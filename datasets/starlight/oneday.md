### OneDay datasets comprise of small longitudial datasets for predicting causality.

#### OneDay New York Geopolitical Events
The OneDay NYGE datasets comprises of a webcrawl each article (title and url) of every New York Times [sitemap](https://www.nytimes.com/sitemap/2025/02/05/) from January 1, 1890 to February 5th, 2025 (today), and an image pdf of every [frontpage](https://static01.nyt.com/images/2025/02/05/nytfrontpage/scan.pdf) of nearby every New York Times from July 6, 2012 to February 5th, 2025 (today). The image pdfs have been transformed using [RemarkableOCR](https://github.com/markelwin/RemarkableOCR) into word tokens and their pixel bounding boxes.

Examples: 
  - Phillies spring training preview: Thoughts on every pitcher headed to camp
  - Trump Officials Tried to Walk Back His Plan to Take Over Gaza  
  - Google Unwinds Diversity Goals, Citing Trump’s DEI Orders
  - Cows Have Been Infected With a Second Form of Bird Flu
  - This Easy Shrimp Dinner Recipe Is Ready in No Time
  - Japan Airlines Plane Clips Delta Plane at Seattle-Tacoma Airport
  - 100,000 Eggs Are Stolen in Pennsylvania Amid Shortage
  - Trump and Netanyahu Leave Little Daylight Between Them

The Onion newspaper famously started each day with its one cardinal rule: a writers' roundtable where every senior writer would read from their list of suggestive headlines before voting on which articles should be written and who should write them. With large language generative models we begin entering the era of questioning how much article content can be generated from a knowledge new worthy headline, and how editors can parameterize and craft those outputs to different reading personalization; but we also must invert our visualization of what this means in terms of knowledge embedded in large scale models: to generate a long-form article from a headline implies reems of specific knowledge implicit in that headline title itself, and therefore the bulk of its article content implicity in the key chosen words of the headline and other available context; the so-called 'world building model' of the language model itself, not unlike the content your brain generates visually and implicitly when you see yourself thinking through the meaning behind those headlines listed above. ⭐ 

These datasets do allow for longitudinal tracking of key events and covariance from subject-noun paring from one element to the next, including event resolution to geographic and named entity actor influence pairs. Their full comprehension of the entire day allows users to ask questions: what are the likelihood that an event of a certain schema will become newsworthy, or even occur, the next day. The inclusion of the full-front page provides more than additional context by providing large scale data to begin asking the question: how much can the raw words of the newspaper scene predict the raw words of the newspaper one day in advance. This dataset will very likely be combined with a similar dataset of books content which introduces the similar question: how many branchings exist from one page to the next when the previous pages of the book are know; or, to regenerate a missing interstitial page. While there are subtle training protocol requirements to prevent the machines from learning to mimic the page already seen in future renders; these are well known problems with well known training protocols.

[sitemaps - uploading tomorrow - link directs to this page](https://github.com/davidbernat/Awesome-Installation-Speedruns)

[frontpages - uploading tomorrow - link directs to this page](https://github.com/davidbernat/Awesome-Installation-Speedruns)

Disclaimer: It is our understanding that these site maps and front pages, as per their short description on their newspaper page, are legal to view and to consume as long as not for product, publication under alternative branding, or advertisement purposes. It is my expectation that New York Times company would inquire to us about research and partnerships to legally codify our work here should their lawyers begin to express concerns that our work here in some way violates the intent or spirit of these legal restrictions on our use of their content here, as we intend to break no laws nor wish to engage in illegal violations or headwinds of the law.
